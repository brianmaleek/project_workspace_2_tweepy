{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "* Create a **300-600 word written report** called \"wrangle_report.pdf\" or \"wrangle_report.html\" that briefly describes your wrangling efforts. This is to be framed as an internal document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "## Introduction\n",
    "\n",
    "Real-world data rarely comes clean. Using Python and its libraries, i'll gather data from a variety of sources and in a variety of formats, assess its quality and tidiness, then clean it. This is called data wrangling. I will document the wrangling efforts in a Jupyter Notebook, plus showcase them through analyses and visualizations using Python (and its libraries).\n",
    "\n",
    "The dataset that will be used for wrangling, analyzing and visualizing is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog.\n",
    "\n",
    "WeRateDogs provided their Twitter archive and sent it to Udacity via email exclusively for use in this project. This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of their tweets as they stood on August 1, 2017.\n",
    "\n",
    "The goal of this project is to wrangle data from WeRateDogs Twitter's account using Python and document the whole process in a Jupyter Notebook named wrangle_act.ipynb i.e. this notebook. My aim is to wrangle this data for interesting and trustworthy analyses using visualizations.\n",
    "\n",
    "## Project Details\n",
    "\n",
    "Fully assessing and cleaning the entire dataset would require exceptional effort so only a subset of its issues (eight quality issues and two tidiness issues at minimum) needed to be assessed and cleaned.The tasks for this project were:\n",
    "\n",
    " 1. Data wrangling, which consists of:\n",
    "    - Gathering data\n",
    "    - Assessing data\n",
    "    - Cleaning data\n",
    " 2. Storing, analyzing, and visualizing our wrangled data\n",
    " 3. Reporting on 1) our data wrangling efforts and 2) our data analyses and visualizations\n",
    "\n",
    "\n",
    "## Gathering Data for this Project\n",
    "### Enhanced Twitter Archive\n",
    "\n",
    "The WeRateDogs Twitter archive is provided by Udacity. This contains basic tweet data for all 5000+ of their tweets, but not everything.Download this file manually by clicking the following link: [twitter_archive_enhanced.csv](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv). Once it is downloaded, upload it and read the data into a pandas DataFrame.\n",
    "\n",
    "### Image Predictions File\n",
    "\n",
    "The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (image_predictions.tsv) is hosted on Udacity's servers and should be downloaded programmatically using the Requests library and the following URL:https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "### Additional Data via the Twitter API\n",
    "Gather **each tweet's retweet count** and **favorite (\"like\") count** at the minimum, and any additional data of interest. Using the tweet IDs in the WeRateDogs Twitter archive, query the Twitter API for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data in a file called `tweet_json.txt` file. Each tweet's JSON data should be written to its own line. Then read this .txt file line by line into a pandas DataFrame with (at minimum) **tweet ID**, **retweet count**, and **favorite count**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Project: Assessing Data\n",
    "### Quality issues\n",
    "#### df_twitter_archive_enhanced\n",
    "\n",
    " 1. change the tweet_id data type is a string object and applies to all the dataframe tables \n",
    " \n",
    " 2. timestamp and  retweeted_status_timestamp should be a datetime type and not an object (string)\n",
    " \n",
    " 3. in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id should be integers or strings instead of float\n",
    " \n",
    " 4. Establish consistency in the rating_denominator column\n",
    "  \n",
    " 5. dogs name is missing, while others have incorrect names like 'a'\n",
    "\n",
    " 6. expanded_urls is missing values \n",
    " \n",
    " 7. Remove retweets by deleting rows with non-null values in retweeted_status_id column\n",
    " \n",
    "\n",
    "#### df_image_prediction\n",
    " 1. P1, P2, P3 dog names are inconsistent in that some have there first letter capitalized while others are not capitalized.\n",
    " \n",
    "#### Tidiness issues to be resolved\n",
    "1. Merging doggo, floofer pupper, puppo columns into one column named dog_type\n",
    "\n",
    "2. df_twitter_archive_enhanced, df_tweet_api, df_image_prediction should be merged into one dataframe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
